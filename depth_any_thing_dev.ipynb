{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72d471d",
   "metadata": {},
   "source": [
    "# Depth Detection Pipeline Notebook\n",
    "This notebook is a development version of the original `depth_any_thing.py` script. It is organized into logical sections for easier experimentation and modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69cf52",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import all necessary libraries for the pipeline, including computer vision, deep learning, and utility modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49098445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import threading\n",
    "from queue import Queue\n",
    "from ultralytics import YOLO\n",
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3811b87",
   "metadata": {},
   "source": [
    "## 2. Configuration & Hyperparameters\n",
    "Define all configuration variables and hyperparameters for the pipeline, including model URLs, video source, and processing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG & HYPERPARAMETERS\n",
    "YOLO_MODEL_URL = \"https://ai-public-videos.s3.us-east-2.amazonaws.com/weights/obb.pt\"\n",
    "VIDEO_URL = \"https://ai-public-videos.s3.us-east-2.amazonaws.com/Raw+Videos/Navirox/sorted/accident_left_2.mp4\"\n",
    "\n",
    "# Performance & Display\n",
    "SCALE_FACTOR = 0.7\n",
    "INFERENCE_RES = (int(400 * SCALE_FACTOR), int(700 * SCALE_FACTOR))  \n",
    "DISPLAY_WIDTH, DISPLAY_HEIGHT = 400, 700\n",
    "FPS = 30  # Targeted output FPS\n",
    "\n",
    "# Depth Calibration\n",
    "ALPHA = 0.15         # Temporal smoothing (lower = smoother)\n",
    "METRIC_FACTOR = 200.0 # Adjust this to calibrate real-world meters\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Async Queues\n",
    "input_queue = Queue(maxsize=5)\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# State for temporal smoothing\n",
    "depth_history = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5e5d8",
   "metadata": {},
   "source": [
    "## 3. Async Video Reader\n",
    "Define the asynchronous video reader function that reads frames from the video source and puts them into a queue for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6765227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_reader(url):\n",
    "    cap = cv2.VideoCapture(url)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[ERROR] Could not open video stream.\")\n",
    "        stop_event.set()\n",
    "        return\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            stop_event.set()\n",
    "            break\n",
    "        \n",
    "        # Pre-resize to model native resolution to save VRAM and transfer time\n",
    "        resized = cv2.resize(frame, INFERENCE_RES)\n",
    "        if not input_queue.full():\n",
    "            input_queue.put(resized)\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c984a15",
   "metadata": {},
   "source": [
    "## 4. Model Initialization\n",
    "Load the YOLO and DepthAnythingV2 models, and move them to the appropriate device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3da562",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Initializing Models on {DEVICE}...\")\n",
    "\n",
    "# Load YOLO\n",
    "yolo_model = YOLO(YOLO_MODEL_URL)\n",
    "\n",
    "# Load DepthAnythingV2 (ViT-B)\n",
    "model_config = {\n",
    "    \"encoder\": \"vitb\",\n",
    "    \"features\": 128,\n",
    "    \"out_channels\": [96, 192, 384, 768]\n",
    "}\n",
    "depth_model = DepthAnythingV2(**model_config)\n",
    "depth_model.load_state_dict(torch.load(\"depth_anything_v2_vitb.pth\", map_location=\"cpu\"))\n",
    "depth_model = depth_model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c10e7",
   "metadata": {},
   "source": [
    "## 5. Main Inference & Rendering Pipeline\n",
    "Define the main function that performs inference, processes detections, applies depth estimation, and renders the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_video = cv2.VideoWriter(\"optimized_maritime_depth.mp4\", fourcc, FPS, (DISPLAY_WIDTH * 2, DISPLAY_HEIGHT))\n",
    "\n",
    "    print(\"[INFO] Inference Running. Press 'q' to quit.\")\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        if input_queue.empty():\n",
    "            continue\n",
    "            \n",
    "        frame = input_queue.get()\n",
    "        \n",
    "        # 1. AI Inference\n",
    "        with torch.no_grad():\n",
    "            # YOLO OBB Detection\n",
    "            results = yolo_model(frame, conf=0.3, iou=0.5, device=0, verbose=False)\n",
    "            # Depth Estimation\n",
    "            depth_map = depth_model.infer_image(frame)\n",
    "\n",
    "        # 2. Distance Normalization\n",
    "        # We normalize to 0-1 range locally for visualization\n",
    "        depth_min, depth_max = depth_map.min(), depth_map.max()\n",
    "        depth_norm = (depth_map - depth_min) / (depth_max - depth_min + 1e-6)\n",
    "        \n",
    "        # Create Colormap\n",
    "        depth_vis = (depth_norm * 255).astype(np.uint8)\n",
    "        depth_colormap = cv2.applyColorMap(depth_vis, cv2.COLORMAP_PLASMA)\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "\n",
    "        # 3. Object Processing with Temporal Smoothing & Metric Correction\n",
    "        for i, (box, cls) in enumerate(zip(results[0].boxes.xyxy, results[0].boxes.cls)):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            class_id = int(cls)\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            \n",
    "            if class_name == 'person': continue # Focusing on boats\n",
    "\n",
    "            # ROI Median Sampling (Robust against waves/spray)\n",
    "            h, w = y2 - y1, x2 - x1\n",
    "            roi = depth_map[y1 + h//4:y2 - h//4, x1 + w//4:x2 - w//4]\n",
    "            \n",
    "            if roi.size > 0:\n",
    "                # INVERSE DEPTH LOGIC: \n",
    "                # Model outputs high values for close things. \n",
    "                # We invert it so small disparities = large distances.\n",
    "                raw_val = np.median(roi)\n",
    "                \n",
    "                # Metric calculation: Distance is inversely proportional to disparity\n",
    "                # We use (1.0 - normalized_value) to ensure horizon (0) = far (1)\n",
    "                norm_val = (raw_val - depth_min) / (depth_max - depth_min + 1e-6)\n",
    "                dist_metric = METRIC_FACTOR / (norm_val + 0.01) \n",
    "\n",
    "                # Temporal Smoothing (EMA)\n",
    "                # Note: Using box index 'i' as temporary ID. \n",
    "                if i not in depth_history:\n",
    "                    depth_history[i] = dist_metric\n",
    "                else:\n",
    "                    depth_history[i] = (ALPHA * dist_metric) + ((1 - ALPHA) * depth_history[i])\n",
    "\n",
    "                smooth_dist = depth_history[i]\n",
    "\n",
    "                # 4. Rendering\n",
    "                color = (0, 255, 0)\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Label with distance\n",
    "                label = f\"{class_name}: {smooth_dist:.1f}m\"\n",
    "                cv2.putText(annotated_frame, label, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "        # 5. Side-by-Side Assembly\n",
    "        res_frame = cv2.resize(annotated_frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "        dep_frame = cv2.resize(depth_colormap, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "        combined = np.hstack((res_frame, dep_frame))\n",
    "\n",
    "        cv2.imshow(\"Maritime Depth Pipeline\", combined)\n",
    "        out_video.write(combined)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            stop_event.set()\n",
    "\n",
    "    out_video.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b8bbf",
   "metadata": {},
   "source": [
    "## 6. Execution Entry Point\n",
    "Start the async video reader thread and run the main pipeline. Handles graceful shutdown on interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc80919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the async reader\n",
    "reader_thread = threading.Thread(target=frame_reader, args=(VIDEO_URL,), daemon=True)\n",
    "reader_thread.start()\n",
    "\n",
    "try:\n",
    "    run_pipeline()\n",
    "except KeyboardInterrupt:\n",
    "    stop_event.set()\n",
    "finally:\n",
    "    stop_event.set()\n",
    "    print(\"[INFO] Pipeline shut down successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
